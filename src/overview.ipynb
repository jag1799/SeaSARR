{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis Overview\n",
    "\n",
    "This notebook includes a general overview of the dataset by (1) preparing your workspace to use the dataset in the COCO format and (2) visualize a few images from one of the datasets for information purposes.  The dataset used is the SARscope dataset found at the below link.  The objective of this project is to determine whether proposed image processing methods would increase the performance of different models on Synthetic Aperture Radar data of maritime vessels.\n",
    "\n",
    "Dataset: https://www.kaggle.com/datasets/kailaspsudheer/sarscope-unveiling-the-maritime-landscape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 - Workspace Preparation\n",
    "\n",
    "To ensure that the user can run this notebook without issue, please do the following:\n",
    "\n",
    "1. Ensure your Python installtion is 3.8.10 or higher.\n",
    "2. You are using the pip3 package manager.\n",
    "3. Run the below installation steps. These are all the packages used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip3 install torch\n",
    "%pip3 install torchvision\n",
    "%pip3 install torchmetrics\n",
    "%pip3 install kagglehub\n",
    "%pip3 install json\n",
    "%pip3 install matplotlib\n",
    "%pip3 install cv2\n",
    "%pip install pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility Imports\n",
    "import os\n",
    "import pathlib\n",
    "import shutil\n",
    "\n",
    "# Data Handling Imports\n",
    "import kagglehub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "\n",
    "# Model & Metric Imports\n",
    "import torch\n",
    "import torchmetrics\n",
    "import torchvision\n",
    "\n",
    "project_path = pathlib.Path.cwd().parent.resolve()\n",
    "print(f\"Project path: {project_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 - Dataset Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.1: Note on Kagglehub\n",
    "\n",
    "Kagglehub does not natively support downloading to specific directories on the user's file system.  It instead downloads it to a cache folder, which may vary between users.  Thus, the below script moves all downloads will move the dataset folder to the included */data* folder in this repo.\n",
    "\n",
    "If you have an error, this is likely due to the `shutil.move()` command failing because it sees the dataset still cached.  To counteract this, `cd` into the cache directory that is printed in the output and delete the entire data folder.  Then run this block again.  See Section 2.2 and the below code block comments for additional information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.2: Deleting the Cache\n",
    "\n",
    "To re-download the dataset, you need to remove both the formatted folder in this repo's data directory (the *kaggle* folder) and the *kailaspsudheer* folder in the cache.\n",
    "\n",
    "**I HIGHLY RECOMMEND YOU DOWNLOAD THE DATA THROUGH KAGGLEPATH AND CLEAR THE CACHE MANUALLY YOUR FIRST TIME.  THIS WILL SHOW YOU WHERE YOUR CACHE IS AND THAT YOUR DELETION PATHS ARE CORRECT.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Kaggle directory to move the downloaded data to\n",
    "kaggle_path = os.path.join(project_path, \"data\", \"kaggle\")\n",
    "\n",
    "# Flag to delete the cached directory so you can re-download the dataset.\n",
    "# NOTE: I recommend you download once with this false to know the following:\n",
    "#   1. Where your cache is.\n",
    "#   2. That the program is finding the \"kailaspsudheer\" directory to delete.\n",
    "#\n",
    "# Once the above is confirmed, you can turn this flag on for future downloads rather than manually deleting it.\n",
    "clear_cache = False\n",
    "\n",
    "if not os.path.exists(kaggle_path):\n",
    "\n",
    "    os.makedirs(kaggle_path, exist_ok=True)\n",
    "\n",
    "    # Download the SARscope dataset from Kaggle\n",
    "    try:\n",
    "        cached_path = kagglehub.dataset_download(\"kailaspsudheer/sarscope-unveiling-the-maritime-landscape\")\n",
    "    except:\n",
    "        raise LookupError(\"Unable to download SEAscope dataset.\")\n",
    "\n",
    "    # Get the absolute path and move it.\n",
    "    cached_path = os.path.abspath(os.path.join(cached_path, \"SARscope\"))\n",
    "\n",
    "    print(f\"Moving cached dataset from directory {cached_path} to {kaggle_path}\")\n",
    "    shutil.move(cached_path, kaggle_path)\n",
    "\n",
    "    data_path = os.path.join(kaggle_path, \"SARscope\")\n",
    "\n",
    "    # Move the annotation files outside the actual data and into their own folder.\n",
    "    annotation_files = []\n",
    "    annotation_folder = os.path.join(data_path, \"annotations\")\n",
    "    print(f\"Making annotations directory at path {annotation_folder}\")\n",
    "    os.makedirs(annotation_folder, exist_ok=True)\n",
    "\n",
    "    for folder in os.listdir(data_path):\n",
    "        # Skip anything that isn't the test, train, or valid directories.\n",
    "        if folder == \"annotations\" or not os.path.isdir(os.path.join(data_path, folder)):\n",
    "            continue\n",
    "        else: # Extract the annotations json file, move it to the annotations directory and rename it according to its corresponding set.\n",
    "            files = os.listdir(os.path.join(data_path, folder))\n",
    "            annotation_file = [x for x in files if x.endswith(\".json\")]\n",
    "\n",
    "            if len(annotation_file) != 1:\n",
    "                raise FileNotFoundError(f\"Annotation file not found for {folder} set.\")\n",
    "\n",
    "            # Rename the annotation file and move it.\n",
    "            new_annotation_file = folder + annotation_file[0]\n",
    "            shutil.move(os.path.join(data_path, folder, annotation_file[0]), os.path.join(annotation_folder, new_annotation_file))\n",
    "\n",
    "    # Show the location of the cache folder.\n",
    "    print(f\"Cached path: {cached_path}\")\n",
    "\n",
    "    if clear_cache:\n",
    "        # Delete the kailaspsudheer directory to allow for a re-download.\n",
    "        split_cached_path = cached_path.split(\"/\")\n",
    "        kail_idx = split_cached_path.index(\"kailaspsudheer\")\n",
    "\n",
    "        kail_dir = '/'.join(split_cached_path[0:kail_idx+1])\n",
    "        print(f\"Deleting cached directory at {kail_dir}\")\n",
    "        shutil.rmtree(kail_dir)\n",
    "else: # Will default to this if you've already downloaded it to the right place.\n",
    "    data_path = os.path.join(project_path, \"data\", \"kaggle\", \"SARscope\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(data_path):\n",
    "    raise FileNotFoundError(f\"Not able to find data directory at path: {data_path}\")\n",
    "else:\n",
    "    print(f\"Using data path: {data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3 - Data Visualization\n",
    "\n",
    "Below, we visualize a few randomly selected images throughout the validation dataset as examples of the different types of images the models will encounter and to ensure the annotations are working as expected.  All targets have the same category Id and category name: (1, \"ship\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Path retrieval\n",
    "val_annotations = os.path.join(data_path, \"annotations\", \"valid_annotations.coco.json\")\n",
    "val_images = os.path.join(data_path, \"valid\")\n",
    "\n",
    "# Extract the annotations\n",
    "coco_annotation = COCO(val_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 3 randomly selected images throughout the validation dataset with their annotations.\n",
    "image_ids = coco_annotation.getImgIds(catIds=[1])\n",
    "id_sample = np.random.choice(image_ids, 3)\n",
    "\n",
    "for i, image_id in enumerate(id_sample):\n",
    "    image_name = coco_annotation.loadImgs([image_id])[0]['file_name']\n",
    "    annotation_id = coco_annotation.getAnnIds(imgIds=image_id, iscrowd=None)\n",
    "    annotation = coco_annotation.loadAnns(annotation_id)\n",
    "\n",
    "    # Open the Image and annotate it.\n",
    "    image = Image.open(os.path.join(val_images, image_name))\n",
    "\n",
    "    plt.imshow(np.asarray(image))\n",
    "    coco_annotation.showAnns(annotation, draw_bbox=True)\n",
    "\n",
    "    plt.title(f\"Image Id: {image_id}\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
